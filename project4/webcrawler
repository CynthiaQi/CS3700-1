#!/usr/bin/python -u
import sys
import socket
import re
from collections import deque

# global variables
HOST = "fring.ccs.neu.edu"
PORT = 80
RECV_BUFFSIZE = 100000 # arbitrarily huge
LOGIN_PATH = "/accounts/login/?next=/fakebook/"
CSRF_TOKEN_REGEX = r'(csrftoken=)(\w{32})'
SESSION_ID_REGEX = r'(sessionid=)(\w{32})'
LOCATION_REGEX = r'(Location: .+.edu)(.+)'

class WebCrawler(object):
    def __init__(self, username, password):
        # login credentials
        self.username = username
        self.password = password
        # cookies
        self.sessionId = ""     # we get this from the initial login process
        self.csrfToken = ""     # we get this from the initial login process
        # socket to reuse
        self.socket = None
        # crawler info to track
        self.pathsToVisit = deque()   # queue of paths to visit
        self.visitedPaths = []        # track to make sure we don't keep visiting same paths
        # flags to track - need to find 5
        self.secretFlags = []

    # initialize and connect to a socket object
    def initSocket(self):
        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.socket.connect((HOST, PORT)) 

    # login and begin crawling process
    def start(self):
        # open socket
        self.initSocket()
        # login
        self.login()
        # TODO crawl server
        #self.crawl()

    # go through login sequence
    def login(self):
        # load initial login page
        response = self.sendGetRequest(LOGIN_PATH)
        # get cookies from response - has set-cookie headers for sessionId and csrfToken
        self.setCookies(response)
        # login to server
        response = self.sendLoginPostRequest()
        # get cookies from response - has set-cookie header for sessionId
        # sessionId doesn't seem to change but project description specifically talks about
        # post login session id specifically
        self.setCookies(response)

    # crawling sequence 
    def crawl(self):
        # start at homepage
        response = sendGetRequest("/fakebook/")
        # add homepage to visitedPaths post login in both forms so we don't revist in a loop
        self.visitedPaths.append("/fakebook/")
        self.visitedPaths.append("/fakebook")

        #TODO parse urls from reponse and add to urls to visit

        while len(self.pathsToVisit) > 0:
            path = self.pathsToVisit[0]
            response = self.sendGetRequest(path)
            responseCode = self.getResponseCode(response)

            if responseCode == 200:
                # TODO parse url paths from response and add to pathsToVisit
                # TODO look for secret codes
                self.pathsToVisit.popLeft()     # remove from queue
                self.visitedPaths.append(path)  # add to visited
                self.areAllFlagsFound()         # check if crawling done
            elif responseCode == 301:
                self.handleRedirect(response)
                self.pathsToVisit.popLeft()     # remove from queue
            elif responseCode == 403 or responseCode == 404:
                self.pathsToVisit.popLeft()     # remove from queue
            elif responseCode == 500:
                # retry the link 
                continue

    # check if all secret flags are found and print them
    def areAllFlagsFound(self):
        if len(self.secretFlags) == 5:
            for flag in self.secretFlags:
                print flag
            # crawler is done so exit
            sys.exit(0)

    # send a get request
    def sendGetRequest(self, path):
        request = self.buildGetRequest(path)
        self.socket.sendall(request)
        response = self.socket.recv(RECV_BUFFSIZE)
        return response

    # build the message for a get request 
    def buildGetRequest(self, path):
        request = "GET " + path + " HTTP/1.1\r\nHost: " + HOST + "\r\nCookie: csrftoken=" 
            + self.csrfToken + "; sessionid=" + self.sessionId + "\r\nConnection:keep-alive\r\n\r\n"
        return request

    # post a login
    def sendLoginPostRequest(self):
        request = self.buildLoginPostRequest()
        self.socket.sendall(request)
        response = self.socket.recv(RECV_BUFFSIZE)
        return response

    # build the request string for a login post 
    def buildLoginPostRequest(self):
        content = "csrfmiddlewaretoken=" + self.csrfToken + "&username=" + self.username 
            + "&password=" + self.password + "&next=%2Ffakebook%2F"
        request = "POST " + LOGIN_PATH + " HTTP/1.1\r\nHost: " + HOST + "\r\nReferrer: " + HOST 
            + LOGIN_PATH + "\r\nCookie: csrftoken=" + self.csrfToken + "; sessionid=" + self.sessionId 
            + "\r\nContent-Type: application/x-www-form-urlencoded\r\nContent-Length: " + len(content) 
            + "\r\nConnection:keep-alive\r\n\r\n" + content + "\r\n"
        return request

    # parse the response for cookies and set them
    def setCookies(self, response):
        csrfToken = re.search(CSRF_TOKEN_REGEX, response)
        sessionId = re.search(SESSION_ID_REGEX, response)

        if csrfToken is not None:
            self.csrfToken = csrfToken.group(2)
        if sessionId is not None:
            self.sessionId = sessionId.group(2)

    # get the response code from the response
    def getResponseCode(self, response):
        if "200 OK" in response:
            return 200
        elif "301 MOVED" in response:
            return 301
        elif "403 FORBIDDEN" in response:
            return 403
        elif "404 NOT FOUND" in response:
            return 404
        elif "500 INTERNAL SERVER ERROR" in response:
            return 500
        else:
            return 200

    def handleRedirect(self, response):
        redirectPath = re.search(LOCATION_REGEX, response)

        if (redirectPath is not None) and (redirectPath.group(2) not in self.pathsToVisit):
            self.pathsToVisit.append(redirectPath.group(2))



if __name__ == '__main__':
    username = argv[1]
    password = argv[2]

    webCrawler = WebCrawler(username, password)
    webCrawler.start()
